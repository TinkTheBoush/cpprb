#+OPTIONS: ':nil -:nil ^:{} num:t toc:nil
#+AUTHOR: Hiroyuki Yamada
#+CREATOR: Emacs 26.1 (Org mode 9.1.14 + ox-hugo)
#+HUGO_WITH_LOCALE:
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: .
#+HUGO_BUNDLE:
#+HUGO_BASE_DIR: ./
#+HUGO_CODE_FENCE:
#+HUGO_USE_CODE_FOR_KBD:
#+HUGO_PREFER_HYPHEN_IN_TAGS:
#+HUGO_ALLOW_SPACES_IN_TAGS:
#+HUGO_AUTO_SET_LASTMOD:
#+HUGO_CUSTOM_FRONT_MATTER:
#+HUGO_BLACKFRIDAY:
#+HUGO_FRONT_MATTER_KEY_REPLACE:
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_PAIRED_SHORTCODES:
#+HUGO_PANDOC_CITATIONS:
#+BIBLIOGRAPHY:
#+HUGO_ALIASES:
#+HUGO_AUDIO:
#+DATE: <2019-02-10 Sun>
#+DESCRIPTION:
#+HUGO_DRAFT:
#+HUGO_EXPIRYDATE:
#+HUGO_HEADLESS:
#+HUGO_IMAGES:
#+HUGO_ISCJKLANGUAGE:
#+KEYWORDS:
#+HUGO_LAYOUT:
#+HUGO_LASTMOD:
#+HUGO_LINKTITLE:
#+HUGO_LOCALE:
#+HUGO_MARKUP:
#+HUGO_MENU:
#+HUGO_MENU_OVERRIDE:
#+HUGO_OUTPUTS:
#+HUGO_PUBLISHDATE:
#+HUGO_SERIES:
#+HUGO_SLUG:
#+HUGO_TAGS:
#+HUGO_CATEGORIES:
#+HUGO_RESOURCES:
#+HUGO_TYPE:
#+HUGO_URL:
#+HUGO_VIDEOS:
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

* Installation
:PROPERTIES:
:EXPORT_HUGO_SECTION*: installation
:END:

** DONE Step by step installation on macOS
CLOSED: [2020-01-17 Fri 21:09]
:PROPERTIES:
:EXPORT_FILE_NAME: install_on_macos
:END:

Since clang (libc++) has not implemented array specialization of
=std::shared_ptr= ([[http://libcxx.llvm.org/cxx1z_status.html][libc++ C++17 Status]], P0414R2 etc.), cpprb cannot be
compiled by clang.

Here we describe how to install cpprb on macOS using [[https://www.macports.org/][MacPorts]].

#+begin_src shell
sudo port selfupdate
sudo port install gcc9
sudo port select gcc mp-gcc9
git clone https://gitlab.com/ymd_h/cpprb.git
cd cpprb
CC=/opt/local/bin/g++ CXX=/opt/local/bin/g++ pip install .
#+end_src

* Features
:PROPERTIES:
:EXPORT_HUGO_SECTION*: features
:END:

** DONE Flexible Environment
CLOSED: [2019-11-08 Fri 05:58]
:PROPERTIES:
:EXPORT_FILE_NAME: flexible_environment
:END:

*** Overview

In ~cpprb~ version 8 and newer, you can store any number of
environments (aka. observation, action, etc.).

For example, you can add your special environments like
~next_next_obs~, ~second_reward~, and so on.

These environments can take multi-dimensional shape (e.g. ~3~,
~(4,4)~, ~(84,84,4)~), and any [[https://numpy.org/devdocs/user/basics.types.html][numpy data type]].


**** ~__init__~
In order to construct replay buffers, you need to specify the second
parameter of their constructor, ~env_dict~.

The ~env_dict~ is a ~dict~ whose keys are environment name and whose
values are ~dict~ describing their properties.

The following table is supported properties and their default values.

| key   | description                    | type                         | default value                                    |
|-------+--------------------------------+------------------------------+--------------------------------------------------|
| shape | shape (size of each dimension) | ~int~ or array like of ~int~ | ~1~                                              |
| dtype | data type                      | ~numpy.dtype~                | ~default_dtype~ in constructor or ~numpy.single~ |

**** ~add~
When ~add~ -ing environments to the replay buffer, you have to pass
them by keyword arguments (aka. ~key=value~ style). If your
environment name is not a syntactically valid identifier, you can
still create dictionary first, then unpack the dictionary by ~**~
operator (e.g. ~rb.add(**kwargs)~).

**** ~sample~
~sample~ returns ~dict~ with keys of environments' name and with
values of sampled ones.


*** Example Usage

#+begin_src python
from cpprb import ReplayBuffer
import numpy as np

buffer_size = 32

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (4,4)},
                               "act": {"shape": 1},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "next_next_obs": {"shape": (4,4)},
                               "done": {},
                               "my_important_info": {"dtype": {np.short}}})

rb.add(obs=np.zeros((4,4)),
       act=1.5,
       rew=0.0,
       next_obs=np.zeros((4,4)),
       next_next_obs=np.zeros((4,4)),
       done=0,
       my_important_info=2)
#+end_src
*** Notes
~priorities~, ~weights~, and ~indexes~ for ~PrioritizedReplayBuffer~
are special environments and are automatically set.


*** Technical Detail
Internally, these flexible environments are implemented with (cython
version of) ~numpy.ndarray~. They were implemented with C++ code in
older than version 8, which had trouble in flexibilities of data type
and the number of environment. (There was a dirty hack to put all
extra environments into ~act~ which was not treat specially.)


** DONE Multistep-add
CLOSED: [2019-11-10 Sun 14:08]
:PROPERTIES:
:EXPORT_FILE_NAME: multistep_add
:END:

*** Overview
In cpprb, you can add multistep environment to replay buffer simultaneously.


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs": {"shape": 5},
                      "act": {"shape": 3},
                      "rew": {},
                      "next_obs": {"shape": 5},
                      "done": {}})

steps = 10

rb.get_stored_size() # -> 0


rb.add(obs=np.ones(steps,5),
       act=np.zeros(steps,3),
       rew=np.ones(steps),
       next_obs=np.ones(steps,5),
       done=np.zeros(steps))


rb.get_stored_size() # -> steps
#+end_src
*** Notes
The dimension for step must be 0th dimension

*** Technical Detail
The shapes for ~add~ for every environments are stored as
~add_shape=(-1,*env_shape)~ at constructor, s.t. ~env_shape~ is the environment
shape.

Only one environment (usually ~done~) is used to determine the step
size by reshaping to ~add_shape~.


** DONE Prioritized Experience Replay
CLOSED: [2019-11-10 Sun 13:26]
:PROPERTIES:
:EXPORT_FILE_NAME: PER
:END:

*** Overview
Prioritized experience replay was proposed by [[https://arxiv.org/abs/1511.05952][T. Schaul et. al.]], and
is widely used to speed up reinforcement learning (as far as I know).

Roughly speaking, mis-predicted observations will be learned more
frequently. To compensate distorted probability, weight of learning is
scaled to the opposite direction (cf. importance sampling).

In cpprb, ~PrioritizedReplayBuffer~ class implements these
functionalities with proportional base (instead of rank base)
priorities.


You can ~add~ ~priorities~ together with other environment. If no
~prioroties~ are passed, the stored maximum priority is used.


The ~dict~ returned by ~sample~ also has special key-values of
~indexes~ and ~weights~. The ~indexes~ are intended to be passed to
~update_priorities~ to update their priorities after comparison with new
prediction.


~PrioritizedReplayBuffer~ has hyperparameters ~alpha~ and ~eps~ at
constructor and ~beta~ at ~sample~, and their default values are
~0.6~, ~1e-4~, and ~0.4~, respectively. The detail is described in the
original paper above.



*** Example Usage
#+begin_src python
from cpprb import PrioritizedReplayBuffer

buffer_size = 256

prb = PrioritizedReplayBuffer(buffer_size,
                              {"obs": {"shape": (4,4)},
                               "act": {"shape": 3},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "done": {}},
                              alpha=0.5)

for i in range(1000):
    prb.add(obs=np.zeros((4,4)),
            act=np.ones(3),
            rew=0.5,
            next_obs=np.zeros(4,4),
            done=0)

batch_size = 32
s = prb.sample(batch_size,beta=0.5)

indexes = s["indexes"]
weights = s["weights"]

#  train
#  ...


per.update_priorities(indexes,new_priorities)
#+end_src
*** Notes

*** Technical Detail
To choose prioritized sample efficiently, partial summation and
minimum of pre-calculated weights are stored in Segment Tree data
structure, which is written by C++ and which was an initial main
motivation of this project.

To support multiprocessing, the Segment Tree can be lazily updated,
too.


** DONE Nstep Experience Replay
CLOSED: [2019-11-10 Sun 13:46]
:PROPERTIES:
:EXPORT_FILE_NAME: nstep
:END:
*** Overview

To reduce fluctuation of random sampling effect especially at
bootstrap phase, N-step reward (discounted summation) are useful.

You can create N-step version replay buffer by specifying ~Nstep~
parameter at constructors of ~ReplayBuffer~ or
~PrioritizedReplayBuffer~.

~Nstep~ parameter is a ~dict~ with keys of ~"size"~ , ~"gamma"~ , and
~"next"~ . ~Nstep["size"]~ is a N-step size and 1-step is identical
with ordinary replay buffer (but inefficient). ~Nstep["gamma"]~ is a
discount factor for reward summation.  ~Nstep["next"]~ , whose type is
~str~ or array like of ~str~, specifies (the set of) next type value(s),
then sample returns (i+N)-th value instead of (i+1)-th one.


~sample~ also adds ~"discount"~ into returned ~dict~.


Since N-step buffer temporary store the values into local storage, you
need to call ~on_episode_end~ member function at the end of the every
episode end to flush into main storage correctly.

*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{'obs': {"shape": (4,4)},
                      'act': {"shape": 3}
                      'rew': {},
                      'next_obs': {"shape": (4,4)}
                      'done': {}},
                  Nstep={"size": 4, "rew": "rew", "next": "next_obs"})

rb.add(obs=np.zeros((4,4)),
       act=np.ones((3)),
       rew=1.0,
       next_obs=np.zeros((4,4)),
       done=0.0)


rb.on_episode_end()
#+end_src

*** Notes

*** Technical Detail

** DONE Memory Compression
CLOSED: [2019-11-10 Sun 13:33]
:PROPERTIES:
:EXPORT_FILE_NAME: memory_compression
:END:

Since replay buffer stores a large number of data set, memory
efficiency is one of the most important point.

In cpprb, there are two functionalities named ~next_of~ and
~stack_compress~, which you can turn on manually when constructing
replay buffer.

~next_of~ and ~stack_compress~ can be used together, but currently
none of them are compatible with N-step replay buffer.


*** ~next_of~

**** Overview
In reinforcement learning, usually a set of observations before and
after a certain action are used for training, so that you save the set
in your replay buffer together. Naively speaking, all observations are
stored twice.

As you know, replay buffer is a ring buffer and the next value should
be stored at next index, except for the newest edge.

If you specify ~next_of~ argument (its type is ~str~ or array like of
~str~), the "next value" of specified values are also set in the
replay buffer and they share the memory location.

The name of the next value adds prefix ~next_~ to the original name
(e.g. ~next_obs~ for ~obs~, ~next_rew~ for ~rew~, and so on).

This functionality has small penalties for manipulating sampled index
and checking the cache for the newest index. (As far as I know, this
penalty is not significant, and you might not notice.)

**** Example Usage
#+begin_src python
from cpprb import ReplayBuffer

buffer_size = 256

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (84,84)},
                               "act": {"shape": 3},
                               "rew": {},
                               "done": {}},
                  next_of=("obs","rew"))

rb.add(obs=np.ones((84,84)),
       act=np.ones(3),
       next_obs=np.ones((84,84)),
       rew=1,
       next_rew=1,
       done=0)
#+end_src

**** Notes
cpprb does not check the consistance of i-th ~next_foo~ and (i+1)-th
~foo~. This is user responsibility.


**** Technical Detail
Internally, ~next_foo~ is not stored into a ring buffer, but into its chache.
(So still raising error if you don't pass them to ~add~.)

When sampling, indices (which is ~numpy.ndarray~) are shifted (and
wraparounded if necessary), then are checked whether they are on the
newest edge of the ring buffer.

*** ~stack_compress~

**** Overview
~stack_compress~ is designed for compressing stacked (or sliding
windowed) observation. A famous use case is Atari video game, where 4
frames of display window are treated as single observation and the
next observation is the one slided by only 1 frame. For this example,
a straight forward approach stores all the frames 4 times.

cpprb stores such stacked observation like non stacked observation
(except for the end edge of the ring buffer) by utilizing numpy
sliding trick.

You can specify ~stack_compress~ parameter, whose type is ~str~ or
array like of ~str~, at constructor.

**** Sample Usage
#+begin_src python
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs":{"shape": (16,16)}, 'rew': {}, 'done': {}},
                  next_of = "obs", stack_compress = "obs")

rb.add(obs=(np.ones((16,16))),
       next_obs=(np.ones((16,16))),
       rew=1,
       done=0)

#+end_src
**** Notes
In order to make compatible with [[https://github.com/openai/gym][OpenAI gym]], the last dimension is
considered as stack dimension (which is not fit to C array memory
order).


**** Technical Detail
Technically speaking ~numpy.ndarray~ (and other data type supporting
buffer protocol) has properties of item data type, the number of
dimensions, length of each dimension, memory step size of each
dimension, and so on. Usually, no data should overlap memory address,
however, ~stack_compress~ intentionally overlaps the memory addresses
in the stacked dimension.



* Contributing
:PROPERTIES:
:EXPORT_HUGO_SECTION*: contributing
:END:

** DONE Step by Step Merge Request
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: merge_request
:END:

The first step of coding contribution is to fork cpprb on GitLab.com.

The detail steps for fork is described at [[https://docs.gitlab.com/ee/gitlab-basics/fork-project.html][official document]].

After fork cpprb on the web, you can clone repository to your local
machine and set original cpprb as "upstream" by

#+begin_src shell
git clone https://gitlab.com/<Your GitLab Account>/cpprb.git
cd cpprb
git remote add upstream https://gitlab.com/ymd_h/cpprb.git
#+end_src

To make "master" branch clean, you need to create new branch before you edit.

#+begin_src shell
git checkout -b <New Branch Name> master
#+end_src

This process is necessay because "master" and other original branches
might progress during your working.


From here, you can edit codes and make commit as usual.


After finish your work, you must recheck original cpprb and ensure
there is no cnflict.

#+begin_src shell
git pull upstream master
git checkout <Your Branch Name>
git merge master # Fix confliction here!
#+end_src


If everything is fine, you push to your cpprb.

#+begin_src shell
git push origin <Your Branch Name>
#+end_src

Merge request can be created from the web, the detail is described at
[[https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html][official document]].


There is [[https://stackoverflow.com/a/14681796][a good explanation]] for making good Pull Request (merge
request equivalent on GitHub.com)

* DONE Examples
CLOSED: [2020-02-15 Sat 09:23]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: examples
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 800
:END:

** Create ~ReplayBuffer~ for non-simple =gym.Env= with helper functions

#+INCLUDE: "../example/create_buffer_with_helper_func.py" src python

* Comparison
:PROPERTIES:
:EXPORT_HUGO_SECTION*: comparison
:EXPORT_HUGO_WEIGHT: 850
:END:

** DONE Comparison
CLOSED: [2020-02-16 Sun 23:08]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:END:

In this section, we compare cpprb with other replay buffer implementations;

- [[https://github.com/openai/baselines][OpenAI Baselines]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.ReplayBuffer]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/ray-project/ray][Ray RLlib]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/optimizers/replay_buffer.py][ray.rilib.optimizers.replay_buffer.ReplayBuffer]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/optimizers/replay_buffer.py][ray.rllib.optimizers.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/chainer/chainerrl][Chainer ChainerRL]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/replay_buffer.py][chainerrl.replay_buffers.ReplayBuffer]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/prioritized.py][chainerrl.replay_buffers.PrioritizedReplayBuffer]]


*Important Notice*

Except cpprb, replay buffers are only a part of their reinforcement
learning ecosystem. These libraries don't focus on providing greatest
replay buffers but reinforcement learning.

Our motivation is to provide strong replay buffers to researchers and
developers who not only use existing networks and/or algorithms but
also creating brand-new networks and/or algorithms.

Here, we would like to show that cpprb is enough functional and enough
efficient compared with others.

** DONE Functionality
CLOSED: [2020-02-24 Mon 12:49]
:PROPERTIES:
:EXPORT_FILE_NAME: functionality
:EXPORT_HUGO_WEIGHT: 10
:END:

The following table summarizes functionalities of replay buffers.

|                           | cpprb                     | OpenAI/Baselines                          | Ray/RLlib                                                  | Chainer/ChainerRL          |
|---------------------------+---------------------------+-------------------------------------------+------------------------------------------------------------+----------------------------|
| *Flexible Environment*    | Yes                       | No                                        | No                                                         | Yes                        |
| *Nstep*                   | Yes                       | No                                        | Yes                                                        | Yes                        |
| *Parellel Exploration*    | No ([[https://gitlab.com/ymd_h/cpprb/-/milestones/3][milestone]], [[https://gitlab.com/ymd_h/cpprb/-/milestones/9][milestone]]) | Yes (Avarages gradients of MPI processes) | Yes (Concatenates sample batches from distributed buffers) | No                         |
| *Save/Load*               | No (Cannot =pickle=)      | No (Maybe can =pickle=)                   | No (Maybe can =pickle=. Trained policies can save/load.)   | Yes                        |
| *Deep Learning Framework* | Anything                  | TensorFlow 1.14 (only this version)       | Anything (Helper functions for [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow][TensorFlow]] and [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch][PyTorch]])     | [[https://chainer.org/][Cahiner]] ([[https://chainer.org/announcement/2019/12/05/released-v7.html][maintenance only]]) |


** DONE Benchmark
CLOSED: [2020-02-16 Sun 23:24]
:PROPERTIES:
:EXPORT_HUGO_WEIGHT: 20
:EXPORT_FILE_NAME: benchmark
:END:

*** Settings

We use following docker image to take benchmarks;

#+INCLUDE: "../benchmark/Dockerfile" src dockerfile

- OpenAI Baselines requires TensorFlow 1.14
- OpenAI Baselines at PyPI seems to be obsolete and requires non-free MuJoCo.
- RLlib requres Pandas, too.


The benchmark script is as follows;

#+INCLUDE: "../benchmark/benchmark.py" src python

Temporary, we excluded Ray/RLlib from =PrioritizedReplayBuffer.sample=
comparison, since we encountered =IndexError= and couldn't fix it.
Probably, there is a bug at around =SegmentTree= usage. (We opened an
[[https://github.com/ray-project/ray/issues/7180][issue]].) After we figured out, we will include it.


*** Results
[[/cpprb/benchmark/ReplayBuffer_add.png]]

[[/cpprb/benchmark/ReplayBuffer_sample.png]]

[[/cpprb/benchmark/PrioritizedReplayBuffer_add.png]]


[[/cpprb/benchmark/PrioritizedReplayBuffer_sample.png]]



* DONE Misc
CLOSED: [2020-01-17 Fri 22:31]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: misc
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 999
:END:

In this section, cpprb related miscellaneous information are described.
