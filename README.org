#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:nil |:t
#+TITLE: cpprb
#+DATE: <2019-01-29 Tue>
#+AUTHOR: Hiroyuki Yamada
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 26.1 (Org mode 9.1.14)

#+HUGO_WITH_LOCALE:
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: overview
#+HUGO_BUNDLE:
#+HUGO_BASE_DIR: ./site
#+HUGO_CODE_FENCE:
#+HUGO_USE_CODE_FOR_KBD:
#+HUGO_PREFER_HYPHEN_IN_TAGS:
#+HUGO_ALLOW_SPACES_IN_TAGS:
#+HUGO_AUTO_SET_LASTMOD:
#+HUGO_CUSTOM_FRONT_MATTER:
#+HUGO_BLACKFRIDAY:
#+HUGO_FRONT_MATTER_KEY_REPLACE:
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_PAIRED_SHORTCODES:
#+HUGO_PANDOC_CITATIONS:
#+BIBLIOGRAPHY:
#+HUGO_ALIASES:
#+HUGO_AUDIO:
#+DESCRIPTION:
#+HUGO_DRAFT:
#+HUGO_EXPIRYDATE:
#+HUGO_HEADLESS:
#+HUGO_IMAGES:
#+HUGO_ISCJKLANGUAGE:
#+KEYWORDS:
#+HUGO_LAYOUT:
#+HUGO_LASTMOD:
#+HUGO_LINKTITLE:
#+HUGO_LOCALE:
#+HUGO_MARKUP:
#+HUGO_MENU:
#+HUGO_MENU_OVERRIDE:
#+HUGO_OUTPUTS:
#+HUGO_PUBLISHDATE:
#+HUGO_SERIES:
#+HUGO_SLUG:
#+HUGO_TAGS:
#+HUGO_CATEGORIES:
#+HUGO_RESOURCES:
#+HUGO_TYPE:
#+HUGO_URL:
#+HUGO_VIDEOS:
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

[[https://img.shields.io/gitlab/pipeline/ymd_h/cpprb.svg]]
[[https://img.shields.io/pypi/v/cpprb.svg]]
[[https://img.shields.io/pypi/l/cpprb.svg]]
[[https://img.shields.io/pypi/status/cpprb.svg]]
[[https://ymd_h.gitlab.io/cpprb/coverage/][https://gitlab.com/ymd_h/cpprb/badges/master/coverage.svg]]

* DONE Overview
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:END:

cpprb is a python ([[https://github.com/python/cpython/tree/master/Python][CPython]]) module providing replay buffer classes for
reinforcement learning.

Major target users are researchers and library developers.

You can build your own reinforcement learning algorithms together with
your favorite deep learning library (e.g. [[https://www.tensorflow.org/][TensorFlow]], [[https://pytorch.org/][PyTorch]]).

cpprb forcuses speed, flexibility, and memory efficiency.

By utilizing [[https://cython.org/][Cython]], complicated calculations (e.g. segment tree for
prioritized experience replay) are offloaded onto C++.
(The name cpprb comes from "C++ Replay Buffer".)

In terms of API, initially cpprb referred to [[https://github.com/openai/baselines][OpenAI baselines]]'s
implementation. In the current version, cpprb has much more
flexibility. Any [[https://numpy.org/][NumPy]] compatible types of any numbers of values can
be stored (as long as memory capacity is sufficient). For example, you
can store the next action and the next next observation, too.


* DONE Installation
:PROPERTIES:
:EXPORT_FILE_NAME: installation
:END:

cpprb requires following softwares before installation.

- C++17 compiler
  - [[https://gcc.gnu.org/][GCC]] (maybe 7.2 and newer)
  - [[https://visualstudio.microsoft.com/][Visual Studio]] (2017 Enterprise is fine)
- Python 3
- pip

Cuurently, [[https://clang.llvm.org/][clang]], which is a default Xcode C/C++ compiler at Apple macOS,
cannot compile cpprb.

If you are macOS user, you need to install GCC and set environment values
of =CC= and =CXX= to =g++=, or just use virtual environment (e.g. [[https://www.docker.com/][Docker]]).


** Install from [[https://pypi.org/][PyPI]] (Recommended)

The following command installs cpprb together with other dependancies.

#+BEGIN_SRC shell
pip install cpprb
#+END_SRC

Depending on your environment, you might need =sudo= or =--user= flag
for installation.

Currently, no binary packages are hosted on PyPI and you need to
compile by your self, however, I plan to distribute binary packages
for Windows, macOS, and Linux in future.

** Install from source code

First, download source code manually or clone the repository;

#+begin_src shell
git clone https://gitlab.com/ymd_h/cpprb.git
#+end_src

Then you can install same way;

#+begin_src shell
cd cpprb
pip install .
#+end_src


For this installation, you need to convert extended Python (.pyx) to
C++ (.cpp) during installation, it takes longer time than installation
from PyPI.


* DONE Usage
:PROPERTIES:
:EXPORT_FILE_NAME: simple_usage
:END:

A simple example is following;
#+BEGIN_SRC python
from cpprb import ReplayBuffer

buffer_size = 256
obs_shape = 3
act_dim = 1
rb = ReplayBuffer(buffer_size,
                  env_dict ={"obs": {"shape": obs_shape},
                             "act": {"shape": act_dim},
                             "rew": {},
                             "next_obs": {"shape": obs_shape},
                             "done": {}})

obs = np.ones(shape=(obs_shape))
act = np.ones(shape=(act_dim))
rew = 0
next_obs = np.ones(shape=(obs_shape))
done = 0

for i in range(500):
    rb.add(obs=obs,act=act,rew=rew,next_obs=next_obs,done=done)


batch_size = 32
sample = rb.sample(batch_size)
# sample is a dictionary whose keys are 'obs', 'act', 'rew', 'next_obs', and 'done'
#+END_SRC

Flexible environment values are defined by =env_dict= when buffer creation.

* DONE Features
:PROPERTIES:
:EXPORT_HUGO_SECTION*: features
:EXPORT_FILE_NAME: _index
:END:

cpprb provides buffer classes for building following algorithms.

| Algorithms                    | cpprb class                                                    | Paper             |
|-------------------------------+----------------------------------------------------------------+-------------------|
| Experience Replay             | =ReplayBuffer=                                                 | [[https://link.springer.com/article/10.1007/BF00992699][L. J. Lin]]         |
| Prioritized Experience Replay | =PrioritizedReplayBuffer=                                      | [[https://arxiv.org/abs/1511.05952][T. Schaul et. al.]] |
| Multi-step Learning           | =ReplayBuffer=, =PrioritizedReplayBuffer= with "Nstep" keyword |                   |





* Classes
=ReplayBuffer= and =PrioritizedReplayBuffer= are supported.

The other classes (including C++ classes) are considered as internal
classes, whose interfaces can change frequently.

** ReplayBuffer
=ReplayBuffer= is a basic replay buffer, where we pick up each time
point randomly. (Duplicated pick up is allowed.)

** PrioritizedReplayBuffer
=PrioritizedReplayBuffer= is a prioritized replay buffer, where you
can set importance (e.g. TD error) to each time point by calling
=PrioritizedReplayBuffer.update_priorities(self,ps)= or
=PrioritizedReplayBuffer.add(self,obs,act,rew,next_obs,done,p)=.
The constructor also take =alpha= parameter, whose default value is =0.6=.
For sampling, you need to pass =beata= argument as well as =batch_size=.

* Links
- Project Site: https://ymd_h.gitlab.io/cpprb/
  - Class Reference: https://ymd_h.gitlab.io/cpprb/api/
  - Unit Test Coverage: https://ymd_h.gitlab.io/cpprb/coverage/
- Main Repositpry: https://gitlab.com/ymd_h/cpprb
- Github Mirror: https://github.com/yamada-github-account/cpprb

