#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:nil todo:nil |:t
#+TITLE: cpprb
#+DATE: <2019-01-29 Tue>
#+AUTHOR: Hiroyuki Yamada
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 26.1 (Org mode 9.1.14)

#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_CUSTOM_FRONT_MATTER: :disableToc true
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: .
#+HUGO_BASE_DIR: ./site
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

[[https://img.shields.io/gitlab/pipeline/ymd_h/cpprb.svg]]
[[https://img.shields.io/pypi/v/cpprb.svg]]
[[https://img.shields.io/pypi/l/cpprb.svg]]
[[https://img.shields.io/pypi/status/cpprb.svg]]
[[https://ymd_h.gitlab.io/cpprb/coverage/][https://gitlab.com/ymd_h/cpprb/badges/master/coverage.svg]]

[[https://pypi.org/project/cpprb/][https://img.shields.io/pypi/dd/cpprb.svg]]
[[https://pypi.org/project/cpprb/][https://img.shields.io/pypi/dw/cpprb.svg]]
[[https://pypi.org/project/cpprb/][https://img.shields.io/pypi/dm/cpprb.svg]]


#+attr_html: :width 100px
[[https://ymd_h.gitlab.io/cpprb/images/favicon.png]]

* Notice

This is modified version of cpprb([[https://github.com/ymd-h/cpprb][orignial]])!!

Minor fixing n_step buffer by my own rules.

This modified cpprb used at [[https://github.com/tinker495/jax-baseline][jax-baseline]].

* Fixed issue

Not all episode end meaning done is true.

So i fixed this issues as below.

** Empty last next step and missing done issue

original cpprb has issue that next step is empty(all 1. states) when episode end.

#+begin_src python
#testing code in haiku-baseline rb

n_step = 4
gamma = 0.995

for i in range(11):
    if i % 20 == 10:
        rb.add([np.arange(i,i+4)], 1, 1, [np.arange(i+4,i+8)], True, False)
        rb.episode_end()
    else:
        rb.add([np.arange(i,i+4)], 1, 1, [np.arange(i+4,i+8)], False, False)
#printing rb transitions
#+END_SRC

#+begin_src python
# original cpprb

obs0: [0. 1. 2. 3.], action: [1.], reward: [3.9701], next_obs0: [ 7.  8.  9. 10.], done: [0.], 
obs0: [1. 2. 3. 4.], action: [1.], reward: [3.9701], next_obs0: [ 8.  9. 10. 11.], done: [0.], 
obs0: [2. 3. 4. 5.], action: [1.], reward: [3.9701], next_obs0: [ 9. 10. 11. 12.], done: [0.], 
obs0: [3. 4. 5. 6.], action: [1.], reward: [3.9701], next_obs0: [10. 11. 12. 13.], done: [0.], 
obs0: [4. 5. 6. 7.], action: [1.], reward: [3.9701], next_obs0: [11. 12. 13. 14.], done: [0.], 
obs0: [5. 6. 7. 8.], action: [1.], reward: [3.9701], next_obs0: [12. 13. 14. 15.], done: [0.], 
obs0: [6. 7. 8. 9.], action: [1.], reward: [3.9701], next_obs0: [13. 14. 15. 16.], done: [0.], 
obs0: [ 7.  8.  9. 10.], action: [1.], reward: [3.9701], next_obs0: [14. 15. 16. 17.], done: [0.], # This step must be done is true!!
obs0: [ 8.  9. 10. 11.], action: [1.], reward: [2.985025], next_obs0: [1. 1. 1. 1.], done: [1.], # empty next step with done
obs0: [ 9. 10. 11. 12.], action: [1.], reward: [1.995], next_obs0: [1. 1. 1. 1.], done: [1.], # empty next step with done
obs0: [10. 11. 12. 13.], action: [1.], reward: [1.], next_obs0: [1. 1. 1. 1.], done: [1.], # empty next step with done

# modified

obs0: [0. 1. 2. 3.], action: [1.], reward: [3.9701], next_obs0: [ 7.  8.  9. 10.], done: [0.], 
obs0: [1. 2. 3. 4.], action: [1.], reward: [3.9701], next_obs0: [ 8.  9. 10. 11.], done: [0.], 
obs0: [2. 3. 4. 5.], action: [1.], reward: [3.9701], next_obs0: [ 9. 10. 11. 12.], done: [0.], 
obs0: [3. 4. 5. 6.], action: [1.], reward: [3.9701], next_obs0: [10. 11. 12. 13.], done: [0.], 
obs0: [4. 5. 6. 7.], action: [1.], reward: [3.9701], next_obs0: [11. 12. 13. 14.], done: [0.], 
obs0: [5. 6. 7. 8.], action: [1.], reward: [3.9701], next_obs0: [12. 13. 14. 15.], done: [0.], 
obs0: [6. 7. 8. 9.], action: [1.], reward: [3.9701], next_obs0: [13. 14. 15. 16.], done: [0.], 
obs0: [ 7.  8.  9. 10.], action: [1.], reward: [3.9701], next_obs0: [14. 15. 16. 17.], done: [1.], 
obs0: [ 8.  9. 10. 11.], action: [1.], reward: [2.985025], next_obs0: [14. 15. 16. 17.], done: [1.], 
obs0: [ 9. 10. 11. 12.], action: [1.], reward: [1.995], next_obs0: [14. 15. 16. 17.], done: [1.], 
obs0: [10. 11. 12. 13.], action: [1.], reward: [1.], next_obs0: [14. 15. 16. 17.], done: [1.],

#+END_SRC

** Undoned terminate episode issue

But what if we have episode end but done is false?

This issue could be happen when we have time limit for episode.

#+begin_src python
#testing code in haiku-baseline rb

n_step = 4
gamma = 0.995

for i in range(11):
    if i % 20 == 10:
        rb.add([np.arange(i,i+4)], 1, 1, [np.arange(i+4,i+8)], False, False)
        rb.episode_end()
    else:
        rb.add([np.arange(i,i+4)], 1, 1, [np.arange(i+4,i+8)], False, False)
#printing rb transitions
#+END_SRC

#+begin_src python
# original cpprb

obs0: [0. 1. 2. 3.], action: [1.], reward: [3.9701], next_obs0: [ 7.  8.  9. 10.], done: [0.], 
obs0: [1. 2. 3. 4.], action: [1.], reward: [3.9701], next_obs0: [ 8.  9. 10. 11.], done: [0.], 
obs0: [2. 3. 4. 5.], action: [1.], reward: [3.9701], next_obs0: [ 9. 10. 11. 12.], done: [0.], 
obs0: [3. 4. 5. 6.], action: [1.], reward: [3.9701], next_obs0: [10. 11. 12. 13.], done: [0.], 
obs0: [4. 5. 6. 7.], action: [1.], reward: [3.9701], next_obs0: [11. 12. 13. 14.], done: [0.], 
obs0: [5. 6. 7. 8.], action: [1.], reward: [3.9701], next_obs0: [12. 13. 14. 15.], done: [0.], 
obs0: [6. 7. 8. 9.], action: [1.], reward: [3.9701], next_obs0: [13. 14. 15. 16.], done: [0.], 
obs0: [ 7.  8.  9. 10.], action: [1.], reward: [3.9701], next_obs0: [14. 15. 16. 17.], done: [0.], # same
obs0: [ 8.  9. 10. 11.], action: [1.], reward: [2.985025], next_obs0: [1. 1. 1. 1.], done: [0.], # error!! next_obs is empty!!
obs0: [ 9. 10. 11. 12.], action: [1.], reward: [1.995], next_obs0: [1. 1. 1. 1.], done: [0.], # error!! next_obs is empty!!
obs0: [10. 11. 12. 13.], action: [1.], reward: [1.], next_obs0: [1. 1. 1. 1.], done: [0.], # error!! next_obs is empty!!

# modified

obs0: [0. 1. 2. 3.], action: [1.], reward: [3.9701], next_obs0: [ 7.  8.  9. 10.], done: [0.], 
obs0: [1. 2. 3. 4.], action: [1.], reward: [3.9701], next_obs0: [ 8.  9. 10. 11.], done: [0.], 
obs0: [2. 3. 4. 5.], action: [1.], reward: [3.9701], next_obs0: [ 9. 10. 11. 12.], done: [0.], 
obs0: [3. 4. 5. 6.], action: [1.], reward: [3.9701], next_obs0: [10. 11. 12. 13.], done: [0.], 
obs0: [4. 5. 6. 7.], action: [1.], reward: [3.9701], next_obs0: [11. 12. 13. 14.], done: [0.], 
obs0: [5. 6. 7. 8.], action: [1.], reward: [3.9701], next_obs0: [12. 13. 14. 15.], done: [0.], 
obs0: [6. 7. 8. 9.], action: [1.], reward: [3.9701], next_obs0: [13. 14. 15. 16.], done: [0.], 
obs0: [ 7.  8.  9. 10.], action: [1.], reward: [3.9701], next_obs0: [14. 15. 16. 17.], done: [0.], # next_obs is over 4 step, gamma^4*q(next_obs) = (1 - done)*gamma^4*q(next_obs)
obs0: [ 8.  9. 10. 11.], action: [1.], reward: [2.985025], next_obs0: [14. 15. 16. 17.], done: [-0.005], # next_obs is over 3 step, gamma^3*q(next_obs) = (1 - done)*gamma^4*q(next_obs)
obs0: [ 9. 10. 11. 12.], action: [1.], reward: [1.995], next_obs0: [14. 15. 16. 17.], done: [-0.00997499],  # next_obs is over 2 step, gamma^2*q(next_obs) = (1 - done)*gamma^4*q(next_obs)
obs0: [10. 11. 12. 13.], action: [1.], reward: [1.], next_obs0: [14. 15. 16. 17.], done: [-0.01492511],  # next_obs is over 1 step, gamma^1*q(next_obs) = (1 - done)*gamma^4*q(next_obs)
#+END_SRC

#+begin_src python
#printing target queue multiplier
for i in range(n,0,-1):
    d = transition[k][-i]
    print(d, (1 - d) * np.power(gamma,n), " = ", np.power(gamma,i))
#+END_SRC

#+begin_src python
[0.] [0.9801495]  =  0.980149500625
[-0.005] [0.98505026]  =  0.985074875
[-0.00997499] [0.98992646]  =  0.990025
[-0.01492511] [0.99477834]  =  0.995
#+END_SRC

* Overview

cpprb is a python ([[https://github.com/python/cpython/tree/master/Python][CPython]]) module providing replay buffer classes for
reinforcement learning.

Major target users are researchers and library developers.

You can build your own reinforcement learning algorithms together with
your favorite deep learning library (e.g. [[https://www.tensorflow.org/][TensorFlow]], [[https://pytorch.org/][PyTorch]]).

cpprb forcuses speed, flexibility, and memory efficiency.

By utilizing [[https://cython.org/][Cython]], complicated calculations (e.g. segment tree for
prioritized experience replay) are offloaded onto C++.
(The name cpprb comes from "C++ Replay Buffer".)

In terms of API, initially cpprb referred to [[https://github.com/openai/baselines][OpenAI Baselines]]'
implementation. The current version of cpprb has much more
flexibility. Any [[https://numpy.org/][NumPy]] compatible types of any numbers of values can
be stored (as long as memory capacity is sufficient). For example, you
can store the next action and the next next observation, too.


* DONE Installation
CLOSED: [2020-01-17 Fri 23:08]
:PROPERTIES:
:EXPORT_HUGO_SECTION: installation
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 100
:END:

cpprb requires following softwares before installation.

- C++17 compiler (for installation from source)
  - [[https://gcc.gnu.org/][GCC]] (maybe 7.2 and newer)
  - [[https://visualstudio.microsoft.com/][Visual Studio]] (2017 Enterprise is fine)
- Python 3
- pip

Additionally, here are user's good feedbacks for installation at [[https://gitlab.com/ymd_h/cpprb/issues/73][Ubuntu]].
(Thanks!)


** Install from [[https://pypi.org/][PyPI]] (Recommended)

The following command installs cpprb together with other dependencies.

#+BEGIN_SRC shell
pip install cpprb
#+END_SRC

Depending on your environment, you might need =sudo= or =--user= flag
for installation.

On supported platflorms (Linux x86-64, Windows amd64, and macOS
x86_64), binary packages hosted on PyPI can be used, so that you don't
need C++ compiler. On the other platforms, such as 32bit or
arm-architectured Linux and Windows, you cannot install from binary,
and you need to compile by yourself. Please be patient, we plan to
support wider platforms in future.


If you have any troubles to install from binary, you can fall back to
source installation by passing ~--no-binary~ option to the above pip
command. (In order to avoid NumPy source installation, it is better to
install NumPy beforehand.)

#+begin_src shell
pip install numpy
pip install --no-binary cpprb
#+end_src

** Install from source code

First, download source code manually or clone the repository;

#+begin_src shell
git clone https://gitlab.com/ymd_h/cpprb.git
#+end_src

Then you can install in the same way;

#+begin_src shell
cd cpprb
pip install .
#+end_src


For this installation, you need to convert extended Python (.pyx) to
C++ (.cpp) during installation, it takes longer time than installation
from PyPI.


* DONE Usage
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_HUGO_SECTION: features
:EXPORT_FILE_NAME: simple_usage
:END:

** Basic Usage
Basic usage is following step;
1. Create replay buffer (~ReplayBuffer.__init__~)
2. Add transitions (~ReplayBuffer.add~)
   1. Reset at episode end (~ReplayBuffer.on_episode_end~)
3. Sample transitions (~ReplayBuffer.sample~)


** Example Code
Here is a simple example for storing standard environment (aka. ~obs~,
~act~, ~rew~, ~next_obs~, and ~done~).

#+BEGIN_SRC python
from cpprb import ReplayBuffer

buffer_size = 256
obs_shape = 3
act_dim = 1
rb = ReplayBuffer(buffer_size,
                  env_dict ={"obs": {"shape": obs_shape},
                             "act": {"shape": act_dim},
                             "rew": {},
                             "next_obs": {"shape": obs_shape},
                             "done": {}})

obs = np.ones(shape=(obs_shape))
act = np.ones(shape=(act_dim))
rew = 0
next_obs = np.ones(shape=(obs_shape))
done = 0

for i in range(500):
    rb.add(obs=obs,act=act,rew=rew,next_obs=next_obs,done=done)

    if done:
        # Together with resetting environment, call ReplayBuffer.on_episode_end()
        rb.on_episode_end()

batch_size = 32
sample = rb.sample(batch_size)
# sample is a dictionary whose keys are 'obs', 'act', 'rew', 'next_obs', and 'done'
#+END_SRC

** Construction Parameters
(See also [[https://ymd_h.gitlab.io/cpprb/api/api/cpprb.ReplayBuffer.html][API reference]])

| Name             | Type                         | Optional           | Discription                       |
|------------------+------------------------------+--------------------+-----------------------------------|
| ~size~           | ~int~                        | No                 | Buffer size                       |
| ~env_dict~       | ~dict~                       | Yes (but unusable) | Environment definition (See [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][here]]) |
| ~next_of~        | ~str~ or array-like of ~str~ | Yes                | Memory compression (See [[https://ymd_h.gitlab.io/cpprb/features/memory_compression/][here]])     |
| ~stack_compress~ | ~str~ or array-like of ~str~ | Yes                | Memory compression (See [[https://ymd_h.gitlab.io/cpprb/features/memory_compression/][here]])     |
| ~default_dtype~  | ~numpy.dtype~                | Yes                | Fall back data type               |
| ~Nstep~          | ~dict~                       | Yes                | Nstep configuration (See [[https://ymd_h.gitlab.io/cpprb/features/nstep/][here]])    |
| ~mmap_prefix~    | ~str~                        | Yes                | mmap file prefix (See [[https://ymd_h.gitlab.io/cpprb/features/mmap/][here]])       |


** Notes
Flexible environment values are defined by =env_dict= when buffer
creation. The detail is described at [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][document]].

Since stored values have flexible name, you have to pass to
=ReplayBuffer.add= member by keyword.


* DONE Features
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_HUGO_SECTION: features
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 200
:END:

cpprb provides buffer classes for building following algorithms.

| Algorithms                            | cpprb class                                  | Paper                  |
|---------------------------------------+----------------------------------------------+------------------------|
| Experience Replay                     | =ReplayBuffer=                               | [[https://link.springer.com/article/10.1007/BF00992699][L. J. Lin]]              |
| [[https://ymd_h.gitlab.io/cpprb/features/per/][Prioritized Experience Replay]]         | =PrioritizedReplayBuffer=                    | [[https://arxiv.org/abs/1511.05952][T. Schaul et. al.]]      |
| [[https://ymd_h.gitlab.io/cpprb/features/nstep/][Multi-step (Nstep) Learning]]           | =ReplayBuffer=, =PrioritizedReplayBuffer=    |                        |
| [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][Multiprocess Learning (Ape-X)]]         | =MPReplayBuffer= =MPPrioritizedReplayBuffer= | [[https://arxiv.org/abs/1803.00933][D. Horgan et. al.]]      |
| [[https://ymd_h.gitlab.io/cpprb/features/laber/][Large Batch Experience Replay (LaBER)]] | =LaBERmean=, =LaBERlazy=, =LaBERmax=         | [[https://dblp.org/db/journals/corr/corr2110.html#journals/corr/abs-2110-01528][T. Lahire et al.]]       |
| [[https://ymd_h.gitlab.io/cpprb/features/rer/][Reverse Experience Replay (RER)]]       | =ReverseReplayBuffer=                        | [[https://arxiv.org/abs/1910.08780][E. Rotinov]]             |
| [[https://ymd_h.gitlab.io/cpprb/features/her/][Hindsight Experience Replay (HER)]]     | =HindsightReplayBuffer=                      | [[https://arxiv.org/abs/1707.01495][M. Andrychowicz et al.]] |




cpprb features and its usage are described at following pages:
- [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][Flexible Environment]]
- [[https://ymd_h.gitlab.io/cpprb/features/multistep_add/][Multi-step add]]
- [[https://ymd_h.gitlab.io/cpprb/features/per/][Prioritized Experience Replay]]
- [[https://ymd_h.gitlab.io/cpprb/features/nstep/][Nstep Experience Replay]]
- [[https://ymd_h.gitlab.io/cpprb/features/memory_compression/][Memory Compression]]
- [[https://ymd_h.gitlab.io/cpprb/features/mmap/][Map Large Data on File]]
- [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][Multiprocess Learning (Ape-X)]]
- [[https://ymd_h.gitlab.io/cpprb/features/save_load_transitions/][Save/Load Transitions]]

* DONE Design
CLOSED: [2021-09-16 Thu 19:09]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: design
:EXPORT_HUGO_WEIGHT: 250
:END:

** Column-oriented and Flexible
One of the most distinctive design of cpprb is column-oriented
flexibly defined transitions. As far as we know, other replay buffer
implementations adopt row-oriented flexible transitions (aka. array of
transition class) or column-oriented non-flexible transitions.

In deep reinforcement learning, sampled batch is divided into
variables (i.e. =obs=, =act=, etc.). If the sampled batch is
row-oriented, users (or library) need to convert it into
column-oriented one. (See [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][doc]], too)

** Batch Insertion
cpprb can accept addition of multiple transitions simultaneously. This
design is convenient when batch transitions are moved from local
buffers to a global buffer. Moreover it is more efficient because of
not only removing pure-Python =for= loop but also suppressing
unnecessary priority updates for PER. (See [[https://ymd_h.gitlab.io/cpprb/features/multistep_add/][doc]], too)

** Minimum Dependency
We try to minimize dependency. Only NumPy is required during its
execution. Small dependency is always preferable to avoid dependency
hell.


* DONE Contributing to cpprb
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_HUGO_SECTION: contributing
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 300
:END:

Any contribution are very welcome!

** Making Community Larger
Bigger commumity makes development more active and improve cpprb.

- Star [[https://gitlab.com/ymd_h/cpprb][GitLab repository]] (and/or [[https://github.com/ymd-h/cpprb][GitHub Mirror]])
- Publish your code using cpprb
- Share this repository to your friend and/or followers.


** Q & A at Forum
When you have any problems or requests, you can check [[https://github.com/ymd-h/cpprb/discussions][Discussions on
GitHub.com]]. If you still cannot find any information, you can post
your own.

We keep [[https://gitlab.com/ymd_h/cpprb/issues][issues on GitLab.com]] and users are still allowed to open
issues, however, we mainly use the place as development issue tracker.

** Merge Request (Pull Request)

cpprb follows local rules:
- Branch Name
  - "HotFix_***" for bug fix
  - "Feature_***" for new feature implementation
- docstring
  - Must for external API
  - [[https://numpydoc.readthedocs.io/en/latest/format.html][Numpy Style]]
- Unit Test
  - Put test code under "test/" directory
  - Can test by =python -m unittest <Your Test Code>= command
  - Continuous Integration on GitLab CI configured by ~.gitlab-ci.yaml~
- Open an issue and associate it to Merge Request

Step by step instruction for beginners is described at [[https://ymd_h.gitlab.io/cpprb/contributing/merge_request][here]].

* DONE Links
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_HUGO_SECTION: misc
:EXPORT_FILE_NAME: links
:END:

** cpprb sites
- [[https://ymd_h.gitlab.io/cpprb/][Project Site]]
  - [[https://ymd_h.gitlab.io/cpprb/api/][Class Reference]]
  - [[https://ymd_h.gitlab.io/cpprb/coverage/][Unit Test Coverage]]
- [[https://gitlab.com/ymd_h/cpprb][Main Repository]]
- [[https://github.com/ymd-h/cpprb][GitHub Mirror]]
- [[https://pypi.org/project/cpprb/][cpprb on PyPI]]

** cpprb users' repositories
- [[https://github.com/keiohta/tf2rl][keiohta/TF2RL]] :: TensorFlow2.x Reinforcement Learning

** Example usage at Kaggle competition
- [[https://www.kaggle.com/ymdhryk/ape-x-dqn-lap-safeguard-rewardredesign][Ape-X DQN-LAP: SafeGuard & RewardRedesign]] | [[https://www.kaggle.com/c/hungry-geese][Hungry Geese]]


** Japanese Documents
- [[https://qiita.com/ymd_h/items/505c607c40cf3e42d080][【強化学習】cpprb で Experience Replay を簡単に！| Qiita]]
- [[https://qiita.com/ymd_h/items/ac9e3f1315d56a1b2718][【強化学習】Ape-X の高速な実装を簡単に！| Qiita]]
- [[https://qiita.com/ymd_h/items/21071d7778cfb3cd596a][【強化学習】自作ライブラリでDQN | Qiita]]
- [[https://zenn.dev/ymd_h/articles/03edcaa47a3b1c][【強化学習】Ape-Xの高速化を実現 | Zenn]]
- [[https://zenn.dev/ymd_h/articles/e65fed3b7991c9][【強化学習】cpprb に遷移のファイル保存機能を追加 | Zenn]]

* DONE License
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_HUGO_SECTION: misc
:EXPORT_FILE_NAME: license
:END:
cpprb is available under MIT license.

#+INCLUDE: "./LICENSE" src plaintext


* DONE Citation
CLOSED: [2021-09-09 Thu 18:05]
:PROPERTIES:
:EXPORT_HUGO_SECTION: misc
:EXPORT_FILE_NAME: citation
:END:

We would be very happy if you cite cpprb in your papers.

#+begin_src bibtex
@misc{Yamada_cpprb_2019,
author = {Yamada, Hiroyuki},
month = {1},
title = {{cpprb}},
url = {https://gitlab.com/ymd_h/cpprb},
year = {2019}
}
#+end_src


- 3rd Party Papers citing cpprb
  - [[https://www.sciencedirect.com/science/article/pii/S187705092100990X][E. Aitygulov and A. I. Panov, "Transfer Learning with Demonstration Forgetting for Robotic Manipulator", Proc. Comp. Sci. 186 (2021), 374-380, https://doi.org/10.1016/j.procs.2021.04.159]]
  - [[https://nips.cc/Conferences/2021/Schedule?showEvent=21848][T. Kitamura and R. Yonetani, "ShinRL: A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives", NeurIPS Deep RL Workshop (2021)]] ([[https://arxiv.org/abs/2112.04123][arXiv]], [[https://github.com/omron-sinicx/ShinRL][code]])
